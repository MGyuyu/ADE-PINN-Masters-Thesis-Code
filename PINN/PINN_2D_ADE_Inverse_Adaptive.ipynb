{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PINN 2D ADE Inverse Adaptive.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MGyuyu/ADE-PINN-Masters-Thesis-Code/blob/main/PINN/PINN_2D_ADE_Inverse_Adaptive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0leyR6bgpo8w"
      },
      "source": [
        "# 2D ADE Inverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD2kwIG2pwZQ"
      },
      "source": [
        "Install needed for latin hypercube sampling (lhs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmASAwF2SWZS",
        "outputId": "b403f953-05f9-4549-ba3d-58a65a059b64"
      },
      "source": [
        "!pip install pyDOE"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyDOE\n",
            "  Downloading pyDOE-0.3.8.zip (22 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyDOE) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyDOE) (1.14.1)\n",
            "Building wheels for collected packages: pyDOE\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18170 sha256=93d34d2737cb281c0f4c27a74a2faef1ca3d93b9b6a8dc9c5babcc0612b9ba3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/20/8c/8bd43ba42b0b6d39ace1219d6da1576e0dac81b12265c4762e\n",
            "Successfully built pyDOE\n",
            "Installing collected packages: pyDOE\n",
            "Successfully installed pyDOE-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPPGM3VlrmC0"
      },
      "source": [
        "All necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLFANE-8TRoB"
      },
      "source": [
        "#@title Imports\n",
        "# Imports related to preprocessing of data and tensorflow\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import tensorflow_probability as tfp\n",
        "from pyDOE import lhs\n",
        "\n",
        "# Imports related to plotting\n",
        "from scipy.io import savemat\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy.interpolate import griddata\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# Imports related to animations\n",
        "import matplotlib.animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_6tJci3saGR"
      },
      "source": [
        "The following two functions are for loading the data and preprocessing. Either through sparse measurements or dense measurements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwvj0XRoZthH"
      },
      "source": [
        "#@title Load data function\n",
        "def LoadData(path, N_u, N_m, noise, inc_bounds, prcnt, sparse, random_pos, timestep, method):\n",
        "\n",
        "  if sparse==False:\n",
        "    # Getting the data with random distribution\n",
        "    x, y, t, X, Y, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb = prep_data_dense(path, N_u, noise, inc_bounds, prcnt)\n",
        "    return x, y, t, X, Y, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb\n",
        "  else:\n",
        "    # Getting the data with sparse distribution\n",
        "    x, y, t, X, Y, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb = prep_data_sparse_measurements(path, N_m, random_pos, timestep, inc_bounds, prcnt, method, noise)\n",
        "    return x, y, t, X, Y, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zge2lj-U8qYV"
      },
      "source": [
        "#@title Data prep sparse\n",
        "def prep_data_sparse_measurements(path, N_m, random_pos, timestep, inc_bounds, prcnt, method, noise):\n",
        "    # Reading external data\n",
        "    data = scipy.io.loadmat(path)\n",
        "\n",
        "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
        "    t = data['timesteps'].flatten()[:,None] # T x 1\n",
        "    y = data['y'].flatten()[:,None] # N x 1\n",
        "    x = data['x'].flatten()[:,None] # N x 1\n",
        "\n",
        "    Nx = x.shape[0]\n",
        "    Ny = y.shape[0]\n",
        "    Nt = t.shape[0]\n",
        "\n",
        "    if random_pos:\n",
        "      rnd_points = np.random.choice(Nx*Ny, N_m, replace=False)\n",
        "    else:\n",
        "      if method == 1:\n",
        "        rnd_points = np.linspace(0,Nx*Ny, N_m, endpoint=False, dtype=np.dtype(int))\n",
        "      if method == 2:\n",
        "        step = np.floor(Nx*Ny/(N_m+1))\n",
        "        rnd_points = np.linspace(step, Nx*Ny-step, N_m, dtype=np.dtype(int))\n",
        "      if method ==3:\n",
        "        rnd_points = np.linspace(0, Nx*Ny-1, N_m, dtype=np.dtype(int))\n",
        "\n",
        "    for i in range(N_m):\n",
        "      next_points = np.arange(rnd_points[i]*Nt, rnd_points[i]*Nt + Nt, timestep)\n",
        "      if i == 0:\n",
        "        sample_idx = next_points\n",
        "      else:\n",
        "        sample_idx = np.append(sample_idx, next_points)\n",
        "\n",
        "    # Keeping the 3D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
        "    Exact_u = np.real(data['u']) # T x N x N\n",
        "\n",
        "    # Meshing x and t in 3D (100,100,2000)\n",
        "    X, Y, T = np.meshgrid(x,y,t)\n",
        "\n",
        "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
        "    X_star = np.hstack((X.flatten()[:,None], Y.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "    # Preparing the testing u_star\n",
        "    u_star = np.swapaxes(Exact_u,0,1)\n",
        "    u_star = u_star.flatten()[:,None]\n",
        "\n",
        "    X_u_train = X_star[sample_idx,:]\n",
        "    u_train = u_star[sample_idx,:]\n",
        "\n",
        "    # Noise\n",
        "    u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
        "    u_star = u_star + noise*np.std(u_star)*np.random.randn(u_star.shape[0], u_star.shape[1])\n",
        "\n",
        "    if inc_bounds == True:\n",
        "      # Getting the initial conditions (t=0)\n",
        "      xx1 = np.hstack((X[:,:,0:1].flatten()[:,None], Y[:,:,0:1].flatten()[:,None], T[:,:,0:1].flatten()[:,None]))\n",
        "      uu1 = Exact_u[:,:,0:1].flatten()[:,None]\n",
        "      # Getting the lowest boundary conditions (x=0)\n",
        "      xx2 = np.hstack((X[0:1,:,:].flatten()[:,None], Y[0:1,:,:].flatten()[:,None], T[0:1,:,:].flatten()[:,None]))\n",
        "      uu2 = Exact_u[0:1,:,:].flatten()[:,None]\n",
        "      # Getting the highest boundary conditions (x=1)\n",
        "      xx3 = np.hstack((X[-1:,:,:].flatten()[:,None], Y[-1:,:,:].flatten()[:,None], T[-1:,:,:].flatten()[:,None]))\n",
        "      uu3 = Exact_u[-1:,:,:].flatten()[:,None]\n",
        "      # Getting the lowest boundary conditions (y=0)\n",
        "      xx4 = np.hstack((X[:,0:1,:].flatten()[:,None], Y[:,0:1,:].flatten()[:,None], T[:,0:1,:].flatten()[:,None]))\n",
        "      uu4 = Exact_u[:,0:1,:].flatten()[:,None]\n",
        "      # Getting the highest boundary conditions (y=1)\n",
        "      xx5 = np.hstack((X[:,-1:,:].flatten()[:,None], Y[:,-1:,:].flatten()[:,None], T[:,-1:,:].flatten()[:,None]))\n",
        "      uu5 = Exact_u[:,-1:,:].flatten()[:,None]\n",
        "      # Selecting random boundary points\n",
        "      X_u_bounds = np.vstack([xx1, xx2, xx3, xx4, xx5])\n",
        "      u_bounds = np.vstack([uu1, uu2, uu3, uu4, uu5])\n",
        "      size = X_u_bounds.shape[0]\n",
        "      samples = int(np.floor(X_u_bounds.shape[0]*prcnt))\n",
        "      rnd_bounds = np.random.choice(size, samples, replace=False)\n",
        "      X_u_bounds = X_u_bounds[rnd_bounds,:]\n",
        "      u_bounds = u_bounds[rnd_bounds,:]\n",
        "      # Stacking them in multidimensional tensors for training\n",
        "      X_u_train = np.vstack([X_u_train, X_u_bounds])\n",
        "      u_train = np.vstack([u_train, u_bounds])\n",
        "\n",
        "    lb = X_star.min(axis=0)\n",
        "    ub = X_star.max(axis=0)\n",
        "    return x, y, t, X, Y, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9hIbcDgTUaV"
      },
      "source": [
        "#@title Data prep dense\n",
        "def prep_data_dense(path, N_u, noise, inc_bounds, prcnt):\n",
        "    # Reading external data\n",
        "    data = scipy.io.loadmat(path)\n",
        "\n",
        "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
        "    t = data['timesteps'].flatten()[:,None] # T x 1\n",
        "    y = data['y'].flatten()[:,None] # N x 1\n",
        "    x = data['x'].flatten()[:,None] # N x 1\n",
        "\n",
        "\n",
        "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
        "    Exact_u = np.real(data['u']) # T x N x N\n",
        "\n",
        "    # Meshing x and t in 2D (100,100,100)\n",
        "    X, Y, T = np.meshgrid(x,y,t)\n",
        "\n",
        "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
        "    X_star = np.hstack((X.flatten()[:,None], Y.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "    # Preparing the testing u_star\n",
        "    u_star = np.swapaxes(Exact_u,0,1)\n",
        "    u_star = u_star.flatten()[:,None]\n",
        "\n",
        "\n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "\n",
        "    # Noise\n",
        "    u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
        "    u_star = u_star + noise*np.std(u_star)*np.random.randn(u_star.shape[0], u_star.shape[1])\n",
        "\n",
        "    if inc_bounds == True:\n",
        "      # Getting the initial conditions (t=0)\n",
        "      xx1 = np.hstack((X[:,:,0:1].flatten()[:,None], Y[:,:,0:1].flatten()[:,None], T[:,:,0:1].flatten()[:,None]))\n",
        "      uu1 = Exact_u[:,:,0:1].flatten()[:,None]\n",
        "      # Getting the lowest boundary conditions (x=0)\n",
        "      xx2 = np.hstack((X[0:1,:,:].flatten()[:,None], Y[0:1,:,:].flatten()[:,None], T[0:1,:,:].flatten()[:,None]))\n",
        "      uu2 = Exact_u[0:1,:,:].flatten()[:,None]\n",
        "      # Getting the highest boundary conditions (x=1)\n",
        "      xx3 = np.hstack((X[-1:,:,:].flatten()[:,None], Y[-1:,:,:].flatten()[:,None], T[-1:,:,:].flatten()[:,None]))\n",
        "      uu3 = Exact_u[-1:,:,:].flatten()[:,None]\n",
        "      # Getting the lowest boundary conditions (y=0)\n",
        "      xx4 = np.hstack((X[:,0:1,:].flatten()[:,None], Y[:,0:1,:].flatten()[:,None], T[:,0:1,:].flatten()[:,None]))\n",
        "      uu4 = Exact_u[:,0:1,:].flatten()[:,None]\n",
        "      # Getting the highest boundary conditions (y=1)\n",
        "      xx5 = np.hstack((X[:,-1:,:].flatten()[:,None], Y[:,-1:,:].flatten()[:,None], T[:,-1:,:].flatten()[:,None]))\n",
        "      uu5 = Exact_u[:,-1:,:].flatten()[:,None]\n",
        "      # Selecting random boundary points\n",
        "      X_u_bounds = np.vstack([xx1, xx2, xx3, xx4, xx5])\n",
        "      u_bounds = np.vstack([uu1, uu2, uu3, uu4, uu5])\n",
        "      size = X_u_bounds.shape[0]\n",
        "      samples = int(np.floor(X_u_bounds.shape[0]*prcnt))\n",
        "      rnd_bounds = np.random.choice(size, samples, replace=False)\n",
        "      X_u_bounds = X_u_bounds[rnd_bounds,:]\n",
        "      u_bounds = u_bounds[rnd_bounds,:]\n",
        "      # Stacking them in multidimensional tensors for training\n",
        "      X_u_train = np.vstack([X_u_train, X_u_bounds])\n",
        "      u_train = np.vstack([u_train, u_bounds])\n",
        "\n",
        "    lb = X_star.min(axis=0)\n",
        "    ub = X_star.max(axis=0)\n",
        "    return x, y, t, X, Y, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekDhsKgYV_KJ"
      },
      "source": [
        "#@title Logger class\n",
        "class Logger(object):\n",
        "    def __init__(self, frequency=10):\n",
        "        print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "        print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "        print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "        self.start_time = time.time()\n",
        "        self.frequency = frequency\n",
        "\n",
        "    def __get_elapsed(self):\n",
        "        return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%H:%M:%S\")\n",
        "\n",
        "    def __get_error_u(self):\n",
        "        return self.error_fn()\n",
        "\n",
        "    def set_error_fn(self, error_fn):\n",
        "        self.error_fn = error_fn\n",
        "\n",
        "    def log_train_start(self, model):\n",
        "        print(\"\\nTraining started\")\n",
        "        print(\"================\")\n",
        "        self.model = model\n",
        "        print(self.model.summary())\n",
        "\n",
        "    def log_train_epoch(self, epoch, loss, current_best_loss, custom=\"\"):\n",
        "        if epoch % self.frequency == 0:\n",
        "            print(f\"{'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  best loss = {current_best_loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "\n",
        "    def log_train_opt(self, name):\n",
        "        print(f\"—— Starting {name} optimization ——\")\n",
        "\n",
        "    def log_train_end(self, epoch, loss, current_best_loss, custom=\"\"):\n",
        "        print(\"==================\")\n",
        "        print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  loss = {loss:.4e}  best loss = {current_best_loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "        print('Applying the weights at the best epoch to the model...')\n",
        "        return self.__get_elapsed()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZqFTnllTo1V"
      },
      "source": [
        "#@title Extract data from PINN\n",
        "def ExtractData(pinn, X_star, X_u_train):\n",
        "\n",
        "  # Getting the model predictions, over dense (x,y,t) domain\n",
        "  X_star_batches = np.array_split(X_star,1000)\n",
        "  u_pred, _ = pinn.predict(X_star_batches[0])\n",
        "  for i in range(len(X_star_batches)-1):\n",
        "    u_pred_batch, _ = pinn.predict(X_star_batches[i+1])\n",
        "    u_pred = np.vstack((u_pred,u_pred_batch))\n",
        "\n",
        "  # Getting the model prediction, over training (x,y,t)\n",
        "  X_u_train_batches = np.array_split(X_u_train,1000)\n",
        "  u_train_pred, _ = pinn.predict(X_u_train_batches[0])\n",
        "  for i in range(len(X_u_train_batches)-1):\n",
        "    u_train_batch, _ = pinn.predict(X_u_train_batches[i+1])\n",
        "    u_train_pred = np.vstack((u_train_pred,u_train_batch))\n",
        "\n",
        "  u_train_pred = u_train_pred.flatten()[:,None]\n",
        "\n",
        "  N_x = x.shape[0]\n",
        "  N_y = y.shape[0]\n",
        "  N_t = t.shape[0]\n",
        "\n",
        "  X_plot = X_star.reshape(N_x,N_y,N_t,3)\n",
        "  u_plot = u_pred.reshape(N_x,N_y,N_t)\n",
        "  u_star_plot = u_star.reshape(N_x,N_y,N_t)\n",
        "  X_test = X_plot[:,:,0,0]\n",
        "  Y_test = X_plot[:,:,0,1]\n",
        "\n",
        "  trained_weights = pinn.get_weights()\n",
        "  pred_params = pinn.get_params(numpy=True)\n",
        "\n",
        "  return X_test, Y_test, u_star_plot, u_plot, u_train_pred, trained_weights, pred_params"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezyP1ZmcY5Y9"
      },
      "source": [
        "#@title Error function\n",
        "def error():\n",
        "    l1, l2, l3 = pinn.get_params(numpy=True)\n",
        "    l1_star, l2_star, l3_star = lambdas_star\n",
        "    error_lambda_1 = np.abs(l1 - l1_star) #/ l1_star\n",
        "    error_lambda_2 = np.abs(l2 - l2_star) #/ l2_star\n",
        "    return (error_lambda_1 + error_lambda_2) / 2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJULzF8coveY"
      },
      "source": [
        "#@title Training data plotter\n",
        "def TrainingDataPlotter(X_u_train, ub, lb):\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "    ax.scatter(X_u_train[:,0],X_u_train[:,1],X_u_train[:,2], marker='o')\n",
        "\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_zlabel('t')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot()\n",
        "\n",
        "    ax.scatter(X_u_train[:,0],X_u_train[:,1], marker='o')\n",
        "\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_xlim(lb[0], ub[0])\n",
        "    ax.set_ylim(lb[1], ub[1])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print('Amount of training data: ' + str(X_u_train.shape[0]))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBU1iSSZSb5l"
      },
      "source": [
        "#@title Discrete plotter\n",
        "def DiscretePlotter(t,X_test,Y_test,u_plot,ub,zmax=0.3,zmin=0.0,cmap='rainbow'):\n",
        "\n",
        "    def figsize(scale, nplots = 1):\n",
        "        fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
        "        inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
        "        golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
        "        fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
        "        fig_height = nplots*fig_width*golden_mean              # height in inches\n",
        "        fig_size = [fig_width,fig_height]\n",
        "        return fig_size\n",
        "\n",
        "    # I make my own newfig and savefig functions\n",
        "    def newfig(width, nplots = 1):\n",
        "        fig = plt.figure(figsize=figsize(width, nplots))\n",
        "        ax = fig.add_subplot(111)\n",
        "        return fig, ax\n",
        "\n",
        "    N_t = t.shape[0]\n",
        "    Lt = ub[2]\n",
        "    t0 = 0; t1 = Lt/5; t2 = 2*Lt/5; t3 = 3*Lt/5; t4 = 4*Lt/5; t5 = 5*Lt/5;\n",
        "    N0 = int(np.ceil((t0*N_t)/Lt))\n",
        "    N1 = int(np.ceil((t1*N_t)/Lt))\n",
        "    N2 = int(np.ceil((t2*N_t)/Lt))\n",
        "    N3 = int(np.ceil((t3*N_t)/Lt))\n",
        "    N4 = int(np.ceil((t4*N_t)/Lt))\n",
        "    N5 = int(np.floor((t5*N_t-1)/Lt))\n",
        "\n",
        "    fig, ax = newfig(3.0, 3.0)\n",
        "    ####### Row 1: u(t,x) slices ##################\n",
        "    gs1 = gridspec.GridSpec(2, 3)\n",
        "    gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "    ax = plt.subplot(gs1[0, 0])\n",
        "    ax.pcolor(X_test, Y_test, u_plot[:,:,N0], cmap='rainbow', vmin=zmin, vmax=zmax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.set_title('$t = 0.0$', fontsize = 10)\n",
        "    ax.axis('square')\n",
        "    #ax.set_xlim([-1.1,1.1])\n",
        "    #ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "    ax = plt.subplot(gs1[0, 1])\n",
        "    ax.pcolor(X_test, Y_test, u_plot[:,:,N1], cmap='rainbow', vmin=zmin, vmax=zmax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.axis('square')\n",
        "    #ax.set_xlim([-1.1,1.1])\n",
        "    #ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 0.2$', fontsize = 10)\n",
        "\n",
        "    ax = plt.subplot(gs1[0, 2])\n",
        "    ax.pcolor(X_test, Y_test, u_plot[:,:,N2], cmap='rainbow', vmin=zmin, vmax=zmax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.axis('square')\n",
        "    #ax.set_xlim([-1.1,1.1])\n",
        "    #ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 0.4$', fontsize = 10)\n",
        "\n",
        "    ax = plt.subplot(gs1[1, 0])\n",
        "    ax.pcolor(X_test, Y_test, u_plot[:,:,N3], cmap='rainbow', vmin=zmin, vmax=zmax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.set_title('$t = 0.6$', fontsize = 10)\n",
        "    ax.axis('square')\n",
        "    #ax.set_xlim([-1.1,1.1])\n",
        "    #ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "    ax = plt.subplot(gs1[1, 1])\n",
        "    ax.pcolor(X_test, Y_test, u_plot[:,:,N4], cmap='rainbow', vmin=zmin, vmax=zmax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.axis('square')\n",
        "    #ax.set_xlim([-1.1,1.1])\n",
        "    #ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 0.8$', fontsize = 10)\n",
        "\n",
        "    ax = plt.subplot(gs1[1, 2])\n",
        "    ax.pcolor(X_test, Y_test, u_plot[:,:,N5], cmap='rainbow', vmin=zmin, vmax=zmax)\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.axis('square')\n",
        "    #ax.set_xlim([-1.1,1.1])\n",
        "    #ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 1.0$', fontsize = 10)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbhF-1l-g11o",
        "cellView": "form"
      },
      "source": [
        "#@title Loss and parameter plotter\n",
        "def LossParameterPlotter(loss_history, l1_history, l2_history, l3_history):\n",
        "    fig, axes = plt.subplots(2, sharex=True, figsize=(12, 12)) # Default plt.subplots(2, sharex=True, figsize=(12, 8))\n",
        "    fig.suptitle('Training Metrics')\n",
        "\n",
        "    axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "    axes[0].set_xlabel(\"Epoch\", fontsize=14)\n",
        "    axes[0].semilogy(loss_history)\n",
        "\n",
        "    axes[1].set_ylabel(\"l1,l2\", fontsize=14)\n",
        "    axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "    axes[1].plot(l1_history)\n",
        "    axes[1].plot(l2_history)\n",
        "    axes[1].plot(l3_history)\n",
        "    axes[1].hlines(lambdas_star[0],0,len(l1_history),linestyles='dashed')\n",
        "    axes[1].hlines(lambdas_star[1],0,len(l2_history),linestyles='dashed')\n",
        "    axes[1].hlines(lambdas_star[2],0,len(l3_history),linestyles='dashed')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlFcluwwCkvt"
      },
      "source": [
        "#@title Parameter loader\n",
        "def ParameterLoader(path):\n",
        "    # Reading external data\n",
        "    data = scipy.io.loadmat(path)\n",
        "\n",
        "    Spread = np.real(data['Spread'])\n",
        "    A  = np.real(data['A'])\n",
        "    Sx = np.real(data['Sx'])\n",
        "    Sy = np.real(data['Sy'])\n",
        "    D  = np.real(data['D'])\n",
        "\n",
        "    return A, D, Spread, Sx, Sy"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cKkcR3jYIae"
      },
      "source": [
        "#@title LBFGS v2\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, state, maxIter = 100, learningRate = 1, do_verbose = True):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  global final_loss, times\n",
        "\n",
        "  maxEval = maxIter*1.25\n",
        "  tolFun = 1e-5\n",
        "  tolX = 1e-9\n",
        "  nCorrection = 50\n",
        "  isverbose = False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "\n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "\n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian\n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "\n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "\n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "\n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "\n",
        "    x += t*d\n",
        "\n",
        "    if nIter != maxIter:\n",
        "    # re-evaluate function only if not in last iteration\n",
        "    # the reason we do this: in a stochastic setting,\n",
        "    # no use to re-evaluate that function here\n",
        "      f, g = opfunc(x)\n",
        "\n",
        "    lsFuncEval = 1\n",
        "    f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      print('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      print('optimality condition below tolFun')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      print('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f,f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      print('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      if nIter % 10 == 0:\n",
        "        print(\"Step %3d loss %6.5f \"%(nIter, f.numpy()))\n",
        "\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNwDHjfabZXy"
      },
      "source": [
        "#@title PINN object w/ Adaptive param\n",
        "class PhysicsInformedNN(object):\n",
        "    def __init__(self, layers, optimizer, logger, ub, lb, f_param, u_param, activation_func=tf.keras.activations.tanh):\n",
        "        # Descriptive Keras model [3, 20, …, 20, 1]\n",
        "        self.u_model = tf.keras.Sequential()\n",
        "        self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "        self.u_model.add(tf.keras.layers.Lambda(\n",
        "          lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "        for width in layers[1:-1]:\n",
        "            self.u_model.add(tf.keras.layers.Dense(\n",
        "              width, activation=activation_func,\n",
        "              kernel_initializer='glorot_normal'))\n",
        "        self.u_model.add(tf.keras.layers.Dense(\n",
        "              layers[-1], activation=None,\n",
        "              kernel_initializer='glorot_normal'))\n",
        "\n",
        "        self.dtype = tf.float32\n",
        "        self.loss_history = []\n",
        "        self.l1_history = []\n",
        "        self.l2_history = []\n",
        "        self.l3_history = []\n",
        "        self.f_param = f_param\n",
        "        self.u_param = u_param\n",
        "        self.current_best_loss = 1e10\n",
        "        self.current_best_weights = []\n",
        "\n",
        "        # Defining the two additional trainable variables for identification\n",
        "        self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
        "        self.lambda_2 = tf.Variable([0.0], dtype=self.dtype)\n",
        "        self.lambda_3 = tf.Variable([0.5], dtype=self.dtype)\n",
        "\n",
        "        self.optimizer = optimizer\n",
        "        self.logger = logger\n",
        "\n",
        "        self.beta = 0.01 # Default 0.1\n",
        "        self.adaptive_param = 1.0\n",
        "        self.adaptive_param_hat = 1.0\n",
        "        self.adaptive_param_history = []\n",
        "\n",
        "        # Computing the sizes of weights/biases for future decomposition\n",
        "        self.sizes_w = []\n",
        "        self.sizes_b = []\n",
        "        for i, width in enumerate(layers):\n",
        "            if i != 1:\n",
        "                self.sizes_w.append(int(width * layers[1]))\n",
        "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    # The actual PINN\n",
        "    def __f_model(self, X_u):\n",
        "        l1, l2, l3 = self.get_params()\n",
        "        # Separating the collocation coordinates\n",
        "        x_f = tf.convert_to_tensor(X_u[:, 0:1], dtype=self.dtype)\n",
        "        y_f = tf.convert_to_tensor(X_u[:, 1:2], dtype=self.dtype)\n",
        "        t_f = tf.convert_to_tensor(X_u[:, 2:3], dtype=self.dtype)\n",
        "\n",
        "        # Using the new GradientTape paradigm of TF2.0,\n",
        "        # which keeps track of operations to get the gradient at runtime\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Watching the two inputs we’ll need later, x and t\n",
        "            tape.watch(x_f)\n",
        "            tape.watch(y_f)\n",
        "            tape.watch(t_f)\n",
        "            # Packing together the inputs\n",
        "            X_f = tf.stack([x_f[:,0], y_f[:,0], t_f[:,0]], axis=1)\n",
        "\n",
        "\n",
        "            # Getting the prediction\n",
        "            u = self.u_model(X_f)\n",
        "            # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
        "            u_x = tape.gradient(u, x_f)\n",
        "            u_y = tape.gradient(u, y_f)\n",
        "\n",
        "        # Getting the other derivatives\n",
        "        u_xx = tape.gradient(u_x, x_f)\n",
        "        u_yy = tape.gradient(u_y, y_f)\n",
        "        u_t = tape.gradient(u, t_f)\n",
        "\n",
        "        # Letting the tape go\n",
        "        del tape\n",
        "\n",
        "        PDE = u_t + 2.0*u_x + (3.0*tf.math.cos(3.0*np.pi*t_f)+1.0)*u_y - 0.2*u_xx - 0.2*u_yy - l3*tf.math.exp(-4.0*tf.math.square(x_f - l1)-4.0*tf.math.square(y_f - l2))\n",
        "\n",
        "        #Buidling the PINNs\n",
        "        return PDE\n",
        "\n",
        "\n",
        "    def __loss(self, X_u, u, u_pred):\n",
        "        f_pred = self.__f_model(X_u)\n",
        "        loss_f = tf.reduce_mean(tf.square(f_pred))\n",
        "        loss_u = tf.reduce_mean(tf.square(u - u_pred))\n",
        "        return loss_u, loss_f\n",
        "\n",
        "    def __grad(self, X, u):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss_u, loss_f = self.__loss(X, u, self.u_model(X))\n",
        "            loss_value = self.adaptive_param*loss_u + 1.0*loss_f\n",
        "        return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "    def __adaptive_param(self, X, u):\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            loss_u, loss_f = self.__loss(X, u, self.u_model(X))\n",
        "            grad_u = tape.gradient(self.adaptive_param*loss_u, self.u_model.trainable_variables, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
        "            grad_f = tape.gradient(loss_f, self.__wrap_training_variables(), unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
        "        del tape\n",
        "\n",
        "        grad_f_flatten = []\n",
        "        grad_u_flatten = []\n",
        "        for g in grad_f:\n",
        "            grad_f_flatten.append(tf.reshape(g,[-1]))\n",
        "        grad_f_flatten = tf.concat(grad_f_flatten,0)\n",
        "        for g in grad_u:\n",
        "            grad_u_flatten.append(tf.reshape(g,[-1]))\n",
        "        grad_u_flatten = tf.concat(grad_u_flatten,0)\n",
        "\n",
        "        max_grad_f = tf.reduce_max(tf.abs(grad_f_flatten))\n",
        "        mean_grad_u = tf.reduce_mean(tf.abs(grad_u_flatten))\n",
        "        self.adaptive_param_hat = max_grad_f / mean_grad_u\n",
        "        self.adaptive_param = self.adaptive_param * (1.0 - self.beta) + self.beta * self.adaptive_param_hat    # As implemented in paper\n",
        "        self.adaptive_param_history.append(self.adaptive_param)\n",
        "\n",
        "    def __wrap_training_variables(self):\n",
        "        var = self.u_model.trainable_variables\n",
        "        var.extend([self.lambda_1, self.lambda_2, self.lambda_3])\n",
        "        return var\n",
        "\n",
        "    def get_loss_and_flat_grad(self, X_u, u):\n",
        "        def loss_and_flat_grad(w):\n",
        "            with tf.GradientTape() as tape:\n",
        "                self.set_weights(w)\n",
        "                loss_u, loss_f = self.__loss(X_u, u, self.u_model(X_u))\n",
        "                loss_value = 1.0*loss_u + 1.0*loss_f                   # According to rescaling LBFGS\n",
        "            grad = tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "            grad_flat = []\n",
        "            for g in grad:\n",
        "                grad_flat.append(tf.reshape(g, [-1]))\n",
        "            grad_flat = tf.concat(grad_flat, 0)\n",
        "            return loss_value, grad_flat\n",
        "        return loss_and_flat_grad\n",
        "\n",
        "    def get_weights(self):\n",
        "        w = []\n",
        "        for layer in self.u_model.layers[1:]:\n",
        "            weights_biases = layer.get_weights()\n",
        "            weights = weights_biases[0].flatten()\n",
        "            biases = weights_biases[1]\n",
        "            w.extend(weights)\n",
        "            w.extend(biases)\n",
        "        w.extend(self.lambda_1.numpy())\n",
        "        w.extend(self.lambda_2.numpy())\n",
        "        w.extend(self.lambda_3.numpy())\n",
        "        return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "    def set_weights(self, w):\n",
        "        for i, layer in enumerate(self.u_model.layers[1:]):\n",
        "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "            weights = w[start_weights:end_weights]\n",
        "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "            biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "            weights_biases = [weights, biases]\n",
        "            layer.set_weights(weights_biases)\n",
        "        self.lambda_1.assign([w[-3]])\n",
        "        self.lambda_2.assign([w[-2]])\n",
        "        self.lambda_3.assign([w[-1]])\n",
        "\n",
        "    def get_params(self, numpy=False):\n",
        "        l1 = self.lambda_1\n",
        "        l2 = self.lambda_2\n",
        "        l3 = self.lambda_3\n",
        "        if numpy:\n",
        "            return l1.numpy()[0], l2.numpy()[0], l3.numpy()[0]\n",
        "        return l1, l2, l3\n",
        "\n",
        "    def summary(self):\n",
        "        return self.u_model.summary()\n",
        "\n",
        "    def mini_batch(self, X_u, u, batch_size):\n",
        "\n",
        "        idx_u = np.random.choice(tf.shape(X_u).numpy()[0], batch_size, replace=False) # Without replacement\n",
        "\n",
        "        X_u_batch = tf.gather(X_u, idx_u)\n",
        "        u_batch   = tf.gather(u, idx_u)\n",
        "\n",
        "        return X_u_batch, u_batch\n",
        "\n",
        "    # The training function\n",
        "    def fit(self, X_u, u, tf_epochs, batch_size=512):\n",
        "        self.logger.log_train_start(self)\n",
        "\n",
        "        # Creating the tensors\n",
        "        X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
        "        u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
        "\n",
        "        iter_per_epoch = int(np.floor(tf.shape(X_u).numpy()[0]/batch_size))\n",
        "\n",
        "        def log_train_epoch(epoch, loss):\n",
        "            l1, l2, l3 = self.get_params(numpy=True)\n",
        "            custom = f\"l1 = {l1:4f} l2 = {l2:4f} l3 = {l3:4f} adaptive_param = {self.adaptive_param:4f}\"\n",
        "            self.logger.log_train_epoch(epoch, loss, self.current_best_loss, custom)\n",
        "        self.logger.log_train_opt(\"Adam\")\n",
        "\n",
        "        for epoch in range(tf_epochs):\n",
        "            for index in range(iter_per_epoch):\n",
        "\n",
        "                X_u_batch, u_batch = self.mini_batch(X_u, u, batch_size)\n",
        "                self.__adaptive_param(X_u_batch, u_batch)\n",
        "\n",
        "                # Optimization step\n",
        "                loss_value, grads = self.__grad(X_u_batch, u_batch)\n",
        "                self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
        "\n",
        "            loss_u, loss_f = self.__loss(X_u, u, self.u_model(X_u))\n",
        "            loss_value = loss_u + loss_f\n",
        "\n",
        "            if loss_value < self.current_best_loss:\n",
        "                self.current_best_loss = loss_value\n",
        "                self.current_best_weights = self.get_weights()\n",
        "\n",
        "            log_train_epoch(epoch, loss_value)\n",
        "\n",
        "            self.loss_history.append(loss_value.numpy())\n",
        "            l1, l2, l3 = self.get_params(numpy=True)\n",
        "            self.l1_history.append(l1)\n",
        "            self.l2_history.append(l2)\n",
        "            self.l3_history.append(l3)\n",
        "\n",
        "        loss_value, _ = self.__grad(X_u, u)\n",
        "        l1, l2, l3 = self.get_params(numpy=True)\n",
        "\n",
        "        #l-bfgs-b optimization\n",
        "        print(\"Starting L-BFGS training\")\n",
        "\n",
        "        loss_and_flat_grad = self.get_loss_and_flat_grad(X_u, u)\n",
        "        lbfgs_weights, lbfgs_history, _ = lbfgs(loss_and_flat_grad, self.get_weights(), Struct(), maxIter=8000, learningRate=0.8)\n",
        "\n",
        "        if lbfgs_history[-1] < self.current_best_loss:\n",
        "            print(\"L-BFGS training was beneficial\")\n",
        "            self.current_best_loss = lbfgs_history[-1]\n",
        "            self.current_best_weights = lbfgs_weights\n",
        "\n",
        "        loss_u, loss_f = self.__loss(X_u, u, self.u_model(X_u))\n",
        "        loss_value = loss_u + loss_f\n",
        "        l1, l2, l3 = self.get_params(numpy=True)\n",
        "\n",
        "        for i in lbfgs_history:\n",
        "            self.loss_history.append(i.numpy())\n",
        "            self.l1_history.append(l1)\n",
        "            self.l2_history.append(l2)\n",
        "            self.l3_history.append(l3)\n",
        "\n",
        "        self.set_weights(self.current_best_weights)\n",
        "        elapsed_time = self.logger.log_train_end(tf_epochs, loss_value, self.current_best_loss, f\"l1 = {l1:4f} l2 = {l2:4f} l3 = {l3:4f} adaptive_param = {self.adaptive_param:4f}\")\n",
        "        return self.loss_history, self.l1_history, self.l2_history, self.l3_history, elapsed_time, self.adaptive_param_history\n",
        "\n",
        "    def predict(self, X_star):\n",
        "        u_star = self.u_model(X_star)\n",
        "        f_star = self.__f_model(X_star)\n",
        "        return u_star.numpy(), f_star.numpy()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTUyPQBKUMC1"
      },
      "source": [
        "############### USER INPUT ##################\n",
        "\n",
        "\n",
        "\n",
        "# Data-points\n",
        "N_u = 52000 # Default 52000 (ONLY FOR DENSE DATA)\n",
        "# Number of measurement stations (ONLY FOR SPARSE DATA)\n",
        "N_m = 26 # Default 26\n",
        "timestep = 1\n",
        "\n",
        "\n",
        "# FNN size\n",
        "layers = [3, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "# Activation function\n",
        "activation_func = tf.keras.activations.tanh\n",
        "\n",
        "\n",
        "# ADAM\n",
        "tf_epochs = 250 # Epochs w/ ADAM\n",
        "learning_rate = 0.1\n",
        "tf_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# True values\n",
        "lambdas_star = (-1.4, -1.0, 1.0)\n",
        "\n",
        "batch_size = 62530 # Full batch = Amount of training data shown below\n",
        "\n",
        "f_param = 0.01 # Default 0.01\n",
        "u_param = 10.0 # Default 10.0\n",
        "\n",
        "# Load data\n",
        "inc_bounds = True\n",
        "random_pos = False\n",
        "sparse = False\n",
        "noise = 0.05\n",
        "prcnt = 0.013 # Amount of boundary points\n",
        "\n",
        "### User needs to input path location. For example: path = os.path.join(\"/folder/2D_ADE_Training_Data.mat\") ###\n",
        "\n",
        "path = os.path.join(\"2D_ADE_v4_renamed.mat\")\n",
        "x, y, t, X, Y, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb = LoadData(path, N_u, N_m, noise, inc_bounds, prcnt, sparse, random_pos, timestep, method=2)\n",
        "\n",
        "TrainingDataPlotter(X_u_train, ub, lb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_8eq4VeUdrP"
      },
      "source": [
        "# Creating the model and training\n",
        "logger = Logger(frequency=1)\n",
        "logger.set_error_fn(error)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, ub, lb, f_param, u_param, activation_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "062Ahsa-UjWF"
      },
      "source": [
        "#loss_history, l1_history, l2_history, l3_history, elapsed_time = pinn.fit(X_u_train, u_train, tf_epochs, batch_size = batch_size)\n",
        "loss_history, l1_history, l2_history, l3_history, elapsed_time, adaptive_param_history = pinn.fit(X_u_train, u_train, tf_epochs, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCebIX6Kh139"
      },
      "source": [
        "LossParameterPlotter(loss_history, l1_history, l2_history, l3_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb8X75lvW6tq"
      },
      "source": [
        "X_test, Y_test, u_star_plot, u_plot, u_train_pred, trained_weights, pred_params = ExtractData(pinn, X_star, X_u_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdCLV7qimjH7"
      },
      "source": [
        "savemat('PINNresultsInverseAdaptiveNm(alpha=0.01).mat', {'u_pred': u_plot, 'u_true': u_star_plot, 'loss_history': loss_history, 'l1_history': l1_history, 'l2_history': l2_history, 'l3_history': l3_history,\n",
        "                           'X_u_train': X_u_train, 'u_train': u_train, 'u_train_pred': u_train_pred, 'X': X_test, 'Y': Y_test,\n",
        "                           'layers': layers, 'Nm': N_m, 'Nu': N_u, 'noise': noise, 'u_param': u_param, 'f_param': f_param, 'batch_size': batch_size, 'learning_rate': learning_rate,\n",
        "                           'adaptive_param': adaptive_param_history})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g_vGuBy06wC"
      },
      "source": [
        "DiscretePlotter(t,X_test,Y_test,u_plot,ub,zmax=0.3,zmin=0.0,cmap='rainbow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unnvj0nK4gVe"
      },
      "source": [
        "DiscretePlotter(t,X_test,Y_test,u_star_plot,ub,zmax=0.3,zmin=0.0,cmap='rainbow')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}